defaults:
  - _self_
  - sac

# env
env: humanoid
gradient_update: 1

# Basic setup
experiment: PEBBLE
agent_name: sac

device: cuda
seed: 0
num_envs: 10

# pretrained model path
pretrained_model_dir: "example/finetuned_model"
pretrained_model_step: 4500000

# training
num_train_steps: 1e7
# replay_buffer_capacity: ${num_train_steps}
replay_buffer_capacity: ${num_train_steps}
num_seed_steps: 10000
num_gradient_update: 1

# use alpha * env reward + beta * learned reward as reward + gamma
reward_alpha: 0.0
reward_beta: 0.5
reward_gamma: 0.5

# reward learning
max_feedback: 1000
reward_batch: 10
segment: 90
activation: tanh
num_interact: 100000 # how often we label preferences
reward_lr: 0.0003
reward_update: 10
feed_type: 2
ensemble_size: 3
large_batch: 20 # in disagreement sampling, we sample reward_batch query pairs from reward_batch * large_batch query pairs

reward_schedule: 0

max_query_save: 1e6
reward_model_capacity: 200
mirror: true

load_reward_model: true 

# logger
log_frequency: 10000
log_save_tb: true

save_model: true
save_model_freq: 100000

# hydra configuration
hydra:
  job:
    name: ${env}
  run:
    dir: ./exp/${env}/finetune_pebble/max_feedback${max_feedback}_reward_batch${reward_batch}_num_interact${num_interact}_segment${segment}_mirror${mirror}_ru${reward_update}_seed${seed}
